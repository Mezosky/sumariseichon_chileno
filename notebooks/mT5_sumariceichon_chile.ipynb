{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a546ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import cuda\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import os\n",
    "\n",
    "# Importing the mT5 modules from huggingface/transformers\n",
    "from transformers import T5Tokenizer, MT5Model, MT5Config, MT5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba74da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loggger\n",
    "logging.basicConfig(\n",
    "    filename = 'finetunning.log', \n",
    "    encoding = 'utf-8', \n",
    "    level    = logging.INFO, \n",
    "    format   = '%(asctime)s :  %(message)s', \n",
    "    datefmt  = '%m/%d/%Y %I:%M:%S %p'\n",
    ")\n",
    "\n",
    "# set up logging to console\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.DEBUG)\n",
    "# set a format which is simpler for console use\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s %(message)s',\n",
    "    datefmt  = '%m/%d/%Y %I:%M:%S %p'\n",
    ")\n",
    "console.setFormatter(formatter)\n",
    "# add the handler to the root logger\n",
    "logging.getLogger('').addHandler(console)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9feb16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params={\n",
    "    \"MODEL\":\"google/mt5-small\",     # model_type: mt5-base/mt5-large\n",
    "    \"TRAIN_BATCH_SIZE\":8,           # training batch size\n",
    "    \"VALID_BATCH_SIZE\":8,           # validation batch size\n",
    "    \"TRAIN_EPOCHS\":2,              # number of training epochs\n",
    "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
    "    \"LEARNING_RATE\":2e-4,           # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\":512,   # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\":258,   # max length of target text\n",
    "    \"SEED\": 42                      # set seed for reproducibility \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99a34a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds and deterministic pytorch for reproducibility\n",
    "torch.manual_seed(model_params[\"SEED\"])\n",
    "np.random.seed(model_params[\"SEED\"]) \n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181af844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryDataSet(Dataset):\n",
    "    \"\"\"\n",
    "    Dataloader to Finetune a mT5 model focused in a summarization task.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        dataframe: pd.DataFrame, \n",
    "        tokenizer: T5Tokenizer, \n",
    "        source_len: int, \n",
    "        target_len: int, \n",
    "        source_text: str, \n",
    "        target_text: str,\n",
    "    ) -> None:\n",
    "        \n",
    "        self.tokenizer   = tokenizer\n",
    "        self.source_len  = source_len\n",
    "        self.summ_len    = target_len\n",
    "        self.target_text = dataframe[target_text]\n",
    "        self.source_text = dataframe[source_text]\n",
    "    \n",
    "    def __getitem__(\n",
    "        self, index: int\n",
    "    ) -> dict[str: torch.Tensor]:\n",
    "        \n",
    "        source_text = str(self.source_text[index])\n",
    "        target_text = str(self.target_text[index])\n",
    "\n",
    "        #cleaning data so as to ensure data is in string type\n",
    "        source_text = ' '.join(source_text.split())\n",
    "        target_text = ' '.join(target_text.split())\n",
    "\n",
    "        source = self.tokenizer.batch_encode_plus(\n",
    "            [source_text], \n",
    "            max_length        = self.source_len, \n",
    "            pad_to_max_length = True, \n",
    "            truncation        = True, \n",
    "            padding           = \"max_length\", \n",
    "            return_tensors    = 'pt'\n",
    "        )\n",
    "        target = self.tokenizer.batch_encode_plus(\n",
    "            [target_text], \n",
    "            max_length        = self.summ_len, \n",
    "            pad_to_max_length = True, \n",
    "            truncation        = True, \n",
    "            padding           = \"max_length\", \n",
    "            return_tensors    = 'pt'\n",
    "        )\n",
    "\n",
    "        source_ids  = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids  = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f72e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: MT5ForConditionalGeneration, \n",
    "    tokenizer: T5Tokenizer, \n",
    "    epoch: int, \n",
    "    loader: SummaryDataSet, \n",
    "    optimizer: torch.optim, \n",
    "    device: str = 'cuda' if cuda.is_available() else 'cpu'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Function to be called for training with the parameters passed from main function\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    for _, data in enumerate(loader, 0):\n",
    "    \n",
    "        y     = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        \n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        ids  = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids         = ids, \n",
    "            attention_mask    = mask, \n",
    "            decoder_input_ids = y_ids, \n",
    "            labels            = lm_labels\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        if _%10==0:\n",
    "            logger.info(f'Epoch: {epoch+1} | Loss: {str(round(float(outputs[0]), 3))}')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827a6512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    model: MT5ForConditionalGeneration,\n",
    "    tokenizer: T5Tokenizer,\n",
    "    epoch: int, \n",
    "    loader: SummaryDataSet,\n",
    "    device: str = 'cuda' if cuda.is_available() else 'cpu'\n",
    ") -> (list, list):    \n",
    "    \"\"\"\n",
    "    Function to evaluate model for predictions\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for it, data in enumerate(loader, 0):\n",
    "            y    = data['target_ids'].to(device, dtype = torch.long)\n",
    "            ids  = data['source_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "              input_ids          = ids,\n",
    "              attention_mask     = mask, \n",
    "              max_length         = 150, \n",
    "              num_beams          = 2,\n",
    "              repetition_penalty = 2.5, \n",
    "              length_penalty     = 1.0, \n",
    "              early_stopping     = True\n",
    "            )\n",
    "\n",
    "            preds  = [\n",
    "                tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) \n",
    "                for g in generated_ids\n",
    "            ]\n",
    "            target = [\n",
    "                tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) \n",
    "                for t in y\n",
    "            ]\n",
    "\n",
    "            if _%10==0:\n",
    "                logger.info(f'Completed {it}')\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            actuals.extend(target)\n",
    "            \n",
    "    return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f3618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mT5Trainer(\n",
    "    dataframe: pd.DataFrame, \n",
    "    source_text: str, \n",
    "    target_text: str, \n",
    "    model_params: dict, \n",
    "    output_dir: str =\"/data/imeza/text_datasets/outputs_mT5/\",\n",
    "    device: str = 'cuda' if cuda.is_available() else 'cpu'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    mT5 trainer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # logging\n",
    "    logger.info(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "    model = MT5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "    model = model.to(device)\n",
    "\n",
    "    # logging\n",
    "    logger.info(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "    # Importing the raw dataset\n",
    "    dataframe = dataframe[[source_text,target_text]]\n",
    "\n",
    "\n",
    "    # Creation of Dataset and Dataloader\n",
    "    # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
    "    train_size = 0.8\n",
    "    train_dataset=dataframe.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
    "    val_dataset=dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "    logger.info(f\"FULL Dataset : {dataframe.shape}\")\n",
    "    logger.info(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "    logger.info(f\"TEST Dataset : {val_dataset.shape}\\n\")\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    training_set = SummaryDataSet(\n",
    "        train_dataset, \n",
    "        tokenizer, \n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"], \n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"], \n",
    "        source_text, target_text\n",
    "    )\n",
    "    val_set      = SummaryDataSet(\n",
    "        val_dataset, tokenizer, \n",
    "        model_params[\"MAX_SOURCE_TEXT_LENGTH\"], \n",
    "        model_params[\"MAX_TARGET_TEXT_LENGTH\"], \n",
    "        source_text, target_text\n",
    "    )\n",
    "\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "      'shuffle': True,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "    val_params = {\n",
    "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "      'shuffle': False,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader      = DataLoader(val_set, **val_params)\n",
    "\n",
    "\n",
    "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    logger.info(f'[Initiating Fine Tuning]...\\n')\n",
    "\n",
    "    for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "        train(model, tokenizer, epoch, training_loader, optimizer, device)\n",
    "\n",
    "    logger.info(f\"[Saving Model]...\\n\")\n",
    "    #Saving the model after training\n",
    "    path = os.path.join(output_dir, \"model_files\")\n",
    "    model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "\n",
    "\n",
    "    # evaluating test dataset\n",
    "    logger.info(f\"[Initiating Validation]...\\n\")\n",
    "    for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
    "        predictions, actuals = validate(model, tokenizer, epoch, val_loader, device)\n",
    "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
    "        final_df.to_csv(os.path.join(output_dir,'predictions.csv'))\n",
    "\n",
    "    logger.info(f\"[Validation Completed.]\\n\")\n",
    "    logger.info(f\"[Model] Model saved @ {os.path.join(output_dir, 'model_files')}\\n\")\n",
    "    logger.info(f\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\")\n",
    "    logger.info(f\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32586c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/data/imeza/text_datasets/data_summarization_with_title.parquet\")\n",
    "df[\"text\"] = \"summarize: \"+df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd844037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09750e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mT5Trainer(dataframe=df, source_text=\"text\", target_text=\"title\", model_params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b30e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/imeza/text_datasets/outputs_mT5/model_files/28'\n",
    "\n",
    "config = MT5Config.from_pretrained(model_params['MODEL'])\n",
    "trained_model = MT5ForConditionalGeneration.from_pretrained(path+\"/pytorch_model.bin\", config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cca0fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\"summarize: La entrega de los premios Musa del domingo pasado habría generado un conflicto que involucra a artistas urbanos como AK4:20, Pailita y Polimá. En la ceremonia, Pailita ganó como Mejor Artista Urbano y junto a Polimá ganaron Mejor Colaboración y Canción del Año por \"Ultra Solo\". Mientras Young Cister también se llevó dos galardones.Sin embargo, en Instagram reportaron que el equipo de AK4:20 tuvo duras palabras para los premios Musa. \"Su premios 'kls' son 'si me das algo te nomino'. Son entero falsos programas 'ktm'\", escribieron en el equipo del hombre de \"Rata tan tan\", terminando con \"Telebasura\". Mientras tanto, el manager del artista Bayron Fire también subió una historia con el texto: \"Premios Musa son más chanta que la ropa de La Polar\". Otro que escribió fue Balbi El Chamako, que entre otras cosas afirmó que \"por culpa de los apitutados o 'wnes' cuicos que tienen cantando a sus regalones (...) ocupan espacios que se los merecían los que de verdad sí lucharon\" y que \"la mafia detrás del género, la desigualdad y el clasismo siempre nos dividirán\". Por su parte, Pailita solo subió un escueto mensaje en su Instagram que dice: \"Si un compañero mío gana, yo también gano. Lo felicito y me alegro por él. Esa es la hermandad\".\"\"\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_params['MODEL'])\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    " \n",
    "generated_ids = trained_model.generate(input_ids, do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0, \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "summary = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "        \"/data/imeza/text_datasets/data_summarization_with_title.parquet\"\n",
    "    ).sample(100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58419ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = \"summarize: \" + df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860402a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "text = df[\"text\"].iloc[i]\n",
    "title = df[\"title\"].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4051ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    " \n",
    "generated_ids = trained_model.generate(input_ids, do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0, \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "summary = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)\n",
    "print(f'\\nTitulo original: {title}')\n",
    "print(f'\\nResumen del modelo: {summary}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nllb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov  4 2022, 13:48:29) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d9df2e7b4f284ad6fa70d894d235a9c33b9c06cad9fa387ca4a85aebedf2e5cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
